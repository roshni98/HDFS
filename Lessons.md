# Lessons

### Rishab Chawla (rc951) and Roshni Shah (rys17)

#### 1. How HDFS works

HDFS is a very complex system that has a lot of moving parts, which can be difficult to understand. On the surface, it seems simple: you have NameNodes, which manage metadata about files, and DataNodes, which store actual file data. Clients can call the NameNode to get or put files on this remote server. However, there are so many optimizations and fault tolerance parameters that we were required to tackle, which allowed us to experience how difficult it can be to build a distributed system. The first 5-10 hours of this project were spent designing the best way to implement this system in Java. We learned firsthand what a hearbeat and blockreport actually does: ensure that the DataNode is alive and the data is persisted properly, respectively. Moreover, the block design of DataNodes was a challenge because what if a DataNode went down while it was writing or a Client was reading from a DataNode. We decided for writes, we would issue a warning and tell the client to resend the request because it conflicted with managing the state of the system. This goes hand in hand with rolling back changes into a database and unassigning that DataNode to manage the blocks for that file. If a Client was reading and the DataNode went down, then we would have to find another DataNode to ensure that the Client could still read. We had to make some challenging assumptions about availability that would not be plausible in a real world infrastructure to keep the project simple. This lightweight version of HDFS was exciting to build because it illustrates the complexity of building a distributed system and the tradeoff between scalability and availability as outlined during lecture. Moreover, I learned that HDFS is a very efficient mechanism for storing files and performing high computer operations on them. In past programming assignments, I have noticed bottlenecks with not chunking files when it comes to accessibility. 100 MB files would take forever to read from because reading millions of bytes of data at once is a bad design. That is too much load for a single packet to handle. Hence, we learned a lot about the underlying architecture of HDFS and how to build fault tolerant distributed systems.

#### 2. Designing a Distributed System

As mentioned earlier, designing a distributed system requires a lot of thinking. We made the smart choice of focusing on the design of the system and drawing diagrams to understand HDFS. It is very difficult to jump right into coding such a daunting task because it requires translating your design to working interfaces and class files that work in an object oriented way. This challenge was very intruging and made us feel like site reliability engineers at Facebook. We used to think these jobs were as simple as clicking a few buttons, but it is much more complicated than this and there is a lot of thought and management that goes into ensuring that these systems do not fail. The most exciting thing about desining this distributed system was being able to deploy it on multiple docker containers at the end.

#### 3. Java Development

While we had significant experience developing with Java in classes such as Software Methodology, we were never challenged to apply everything we learned and beyond into one large project. We learned a lot about interfaces in Java and how to make Remote Proceduce calls using Java's RMI library. Moreover, we learned more about marshalling data structures such as hashmaps using protobuffers and using these protobuffers to service multiple requests between DataNodes and NameNodes. We also had very limited experience with threads in Java. Not only are we able to send heartbeats and block reports at a specified time interval, but also we can service multiple clients. Also, we learned a lot about objects and how to create local objects that represent an actual object running on a virtual machine (for example DataNode objects in NameNode). While developing Java was not a challenge, we were still able to learn about RPCs, threading, and best OOP practices.

#### 4. Docker

We had heard the name Docker before but never understood what it was or how it worked. Not only were we able to run a NameNode and DataNode cluster on docker, but we learned a lot about compiling Dockerfiles and using Docker containers as a lightweight OS that can perform high compute functions. This made testing our code on a remote server easier and satisfying to watch.

#### 5. Github

We had a lot of merge conflict on Github because we used branches to split development. We learned a lot about how to manage these conflicts from the Command Line Interface using pull requests. This showed us the power of git.
